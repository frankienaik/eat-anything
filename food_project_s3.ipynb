{"cells":[{"cell_type":"code","source":["import urllib2\nimport json\ninstance = json.loads(urllib2.urlopen('http://169.254.169.254/latest/dynamic/instance-identity/document').read())\nprint(instance)\n\nimport urllib2\npublic_hostname = urllib2.urlopen('http://169.254.169.254/latest/meta-data/public-hostname').read()\npublic_hostname\n\nimport urllib2\nimport json\n\n# modify this to be the number of Workers you have running (in Clusters UI)\nnumWorkers = 10 \n\nworker_instance_ids = sc.parallelize(xrange(numWorkers)).map(lambda x: \n  (urllib2.urlopen('http://169.254.169.254/latest/meta-data/public-hostname').read(), urllib2.urlopen('http://169.254.169.254/latest/meta-data/public-ipv4').read())\n)\n\nprint worker_instance_ids.distinct().collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{u&apos;devpayProductCodes&apos;: None, u&apos;availabilityZone&apos;: u&apos;us-west-2c&apos;, u&apos;instanceId&apos;: u&apos;i-09a278efe8972d62d&apos;, u&apos;pendingTime&apos;: u&apos;2018-12-06T07:51:56Z&apos;, u&apos;marketplaceProductCodes&apos;: None, u&apos;region&apos;: u&apos;us-west-2&apos;, u&apos;privateIp&apos;: u&apos;10.172.247.207&apos;, u&apos;version&apos;: u&apos;2017-09-30&apos;, u&apos;architecture&apos;: u&apos;x86_64&apos;, u&apos;billingProducts&apos;: None, u&apos;kernelId&apos;: None, u&apos;ramdiskId&apos;: None, u&apos;imageId&apos;: u&apos;ami-09a49d3517e37f7c2&apos;, u&apos;instanceType&apos;: u&apos;r3.2xlarge&apos;, u&apos;accountId&apos;: u&apos;955181726619&apos;}\n[(&apos;ec2-54-190-67-5.us-west-2.compute.amazonaws.com&apos;, &apos;54.190.67.5&apos;)]\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["\nACCESS_KEY = \nSECRET_KEY = \n#BUCKET_NAME = \"food-recipe-data\"\nMOUNT_NAME = \n\n\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"food-recipe-data\"\nMOUNT_NAME = \"mount-name\"\n\ntry:  \n  dbutils.fs.unmount(\"/mnt/mount-name\")\nexcept:\n  pass\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/mount-name has been unmounted.\n<span class=\"ansired\">Out[</span><span class=\"ansired\">81</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["\nrecipes_DF = spark.read.json(\"/mnt/%s/train.json\" % MOUNT_NAME,multiLine=True)\n#recipes_DF = spark.read.json(\"s3a://food-recipe-data/train.json\",multiLine=True)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-4263442112024196&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>recipes_DF <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>json<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;/mnt/%s/train.json&quot;</span> <span class=\"ansiyellow\">%</span> MOUNT_NAME<span class=\"ansiyellow\">,</span>multiLine<span class=\"ansiyellow\">=</span>True<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> <span class=\"ansired\">#recipes_DF = spark.read.json(&quot;s3a://food-recipe-data/train.json&quot;,multiLine=True)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansicyan\">json</span><span class=\"ansiblue\">(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, encoding)</span>\n<span class=\"ansigreen\">    265</span>             path <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>path<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    266</span>         <span class=\"ansigreen\">if</span> type<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">==</span> list<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 267</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_df<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jreader<span class=\"ansiyellow\">.</span>json<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_spark<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonUtils<span class=\"ansiyellow\">.</span>toSeq<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    268</span>         <span class=\"ansigreen\">elif</span> isinstance<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">,</span> RDD<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    269</span>             <span class=\"ansigreen\">def</span> func<span class=\"ansiyellow\">(</span>iterator<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o17731.json.\n: java.io.IOException: Bucket food-recipe-data does not exist\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:95)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:55)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.getFileStatus(DatabricksFileSystemV1.scala:260)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:213)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:775)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$16.apply(DataSource.scala:410)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$16.apply(DataSource.scala:410)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:303)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:291)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:466)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Bucket food-recipe-data does not exist\n\tat com.databricks.s3a.S3AFileSystem.initialize(S3AFileSystem.java:316)\n\tat com.databricks.backend.daemon.data.server.backend.S3AFSBackend.fs$lzycompute(S3AFSBackend.scala:37)\n\tat com.databricks.backend.daemon.data.server.backend.S3AFSBackend.fs(S3AFSBackend.scala:31)\n\tat com.databricks.backend.daemon.data.server.backend.S3AFSBackend.fs(S3AFSBackend.scala:21)\n\tat com.databricks.backend.daemon.data.server.backend.HadoopFSBackend.getFileStatus(HadoopFSBackend.scala:42)\n\tat com.databricks.backend.daemon.data.server.backend.RootFileSystemBackend.getFileStatus(RootFileSystemBackend.scala:55)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.$anonfun$receive$2(FileSystemRequestHandler.scala:33)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$2(UsageLogging.scala:359)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:227)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionContext(FileSystemRequestHandler.scala:19)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:268)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:264)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionTags(FileSystemRequestHandler.scala:19)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:345)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:320)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperation(FileSystemRequestHandler.scala:19)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.receive(FileSystemRequestHandler.scala:33)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:83)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:82)\n\tat scala.collection.immutable.List.foreach(List.scala:388)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:82)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:272)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:252)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:42)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:58)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:58)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:38)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$2(UsageLogging.scala:359)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:227)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:268)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:264)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:345)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:320)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:297)\n\tat scala.util.Try$.apply(Try.scala:209)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:297)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:230)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:164)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:227)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:91)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:268)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:264)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:91)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:155)\n\tat com.databricks.rpc.JettyServer$RequestManager.doGet(JettyServer.scala:109)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# INSPECT THE SIZE OF OUR DATA\n\nimport re\n\ndef to_standard(string):\n  return string.replace(\"_\", \" \").upper().encode(\"utf-8\") \n\nn_recipes_by_cuisine_RDD = (recipes_DF.rdd\n                            .map(lambda r : (to_standard(r[\"cuisine\"]), 1))\n                            .reduceByKey(lambda a, b : a + b)\n                            .cache())\n\nn_occurences_per_ingredient_RDD = (recipes_DF.rdd\n                                   .flatMap(lambda r : r[\"ingredients\"])\n                                   .map(lambda i : (to_standard(i), 1))\n                                   .reduceByKey(lambda a, b : a + b)\n                                   .cache())\n\navg_ingredients_by_cuisine_RDD = (recipes_DF.rdd\n                                  .map(lambda r : (to_standard(r[\"cuisine\"]), (len(r[\"ingredients\"]), 1)))\n                                  .reduceByKey(lambda a, b : (a[0] + b[0], a[1] + b[1]))\n                                  .map(lambda (c, t) : (c, float(t[0]) / t[1]))\n                                  .cache())\n\nprint \"No. of Recipes: %s\" % recipes_DF.rdd.count()\nprint \"No. of Cuisines: %s\" % n_recipes_by_cuisine_RDD.count()\nprint \"No. of Ingredients: %s\" % n_occurences_per_ingredient_RDD.count()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def check_similarity(a, b):\n  x = results.get(a)\n  y = results.get(b)\n  \n  if x == None or y == None:\n    return -1\n  else:\n    return x.dot(y) / (x.norm(2) * y.norm(2))\n\nN = 10\nM = 3\n\nto_compare_RDD = (n_occurences_per_ingredient_RDD\n              .sortBy(lambda (i, sum) : 1 * sum))\n\nto_compare = [t[0] for t in to_compare_RDD.take(N)]\n\noutput = []\nfor ingredient in to_compare:\n  cosine_similarities_RDD = (n_occurences_per_ingredient_RDD\n                             .map(lambda (i, sum) : i)\n                             .map(lambda i : (i, check_similarity(ingredient, i))))\n  \n  most_similar = cosine_similarities_RDD.takeOrdered(M + 1, lambda (i, similarity) : -1 * similarity)\n  \n  string = \"%s -\" % ingredient\n  for i, similarity in most_similar:\n    if not i == ingredient:\n      string += \"\\n\\t%s: %s\" % (i, round(similarity, 3))\n  \n  output.append(string)\n\nprint \"\\n\\n\".join(output)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# SEPARATE DATA INTO TRAINING AND TEST SETS\n\nWEIGHTS = [0.99, 0.01]\n\ntrain_recipes_RDD, test_recipes_RDD = (recipes_DF.rdd\n                                       .randomSplit(WEIGHTS))\n\ntrain_recipes_RDD = (train_recipes_RDD\n                     .map(lambda r : (to_standard(r[\"cuisine\"]), {to_standard(i): 1 for i in r[\"ingredients\"]}))\n                     .cache())\n\ntest_recipes_RDD = (test_recipes_RDD\n                    .map(lambda r : (to_standard(r[\"cuisine\"]), [to_standard(i) for i in r[\"ingredients\"]]))\n                    .cache())\n\nprint \"No. of Samples in Training Set: %s\" % train_recipes_RDD.count()\nprint \"No. of Samples in Test Set: %s\" % test_recipes_RDD.count()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# CALCULATE NORMALIZED RELATIVE FREQUENCIES PER INGREDIENT, BY CUISINE\n\ndef combine_dict(this, other):\n  for key, value in other.iteritems():\n    if this.has_key(key):\n      this[key] += other[key]\n    else:\n      this[key] = other[key]\n      \n  return this\n\nnormalized_frequencies_RDD = (train_recipes_RDD\n                              .reduceByKey(lambda a, b : combine_dict(a, b))\n                              .map(lambda (c, i) : (c, {k: len(i) * float(v) / sum(i.values()) for k, v in i.iteritems()}))\n                              .cache())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# TAKE A LOOK AT THE VALUES (SANITY CHECK)\n\nORDER = -1\n\ndef sort_by_value(dict):\n  sorted_keys = sorted(dict, key=lambda x : ORDER * dict[x])\n  output = []\n  \n  for key in sorted_keys:\n      output.append((key, dict[key]))\n      \n  return output\n\n#sorted_aggregated_ingredients_RDD = (aggregated_ingredients_RDD\n#                                     .map(lambda (c, i) : (c, sort_by_value(i))))\n\nsorted_normalized_frequencies_RDD = (normalized_frequencies_RDD\n                                     .map(lambda (c, i) : (c, sort_by_value(i))))\n\ndef display_frequencies(dict, n=5):\n  output = []\n  for cuisine, ingredients in dict:\n    string = \"%s -\" % cuisine\n    for i in range(n):\n      string += \"\\n\\t%s: %s\" % (ingredients[i][0], ingredients[i][1])     \n    output.append(string)\n    \n  print \"\\n\\n\".join(output)\n\nn = 5\n\nprint \"RAW FREQUENCIES (TOP %s INGREDIENTS)\" % n\nprint \"-----------------------------------\"\n#display_frequencies(sorted_aggregated_ingredients_RDD.collect(), n)\n\nprint \"\"\n\nprint \"NORMALIZED RELATIVE FREQUENCIES (TOP %s INGREDIENTS)\" %n\nprint \"---------------------------------------------------\"\ndisplay_frequencies(sorted_normalized_frequencies_RDD.collect(), n)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# 'FORWARD PROPAGATE' SINGLE-LAYER NETWORK AND CALCULATE ACCURACY\n\ndef calculate_class_value(frequencies, ingredients):\n  sum = 0\n  for i in ingredients:\n    if i in frequencies:\n      print i\n      sum += frequencies[i]\n  \n  return sum\n\nsum_correct = 0\nsum_in_top_3 = 0\n\nfor cuisine, recipe in test_recipes_RDD.collect():\n  output_RDD = (normalized_frequencies_RDD\n                .map(lambda (c, i) : (c, calculate_class_value(i, recipe)))\n                .cache())\n  \n  predictions = [c for c, value in output_RDD.takeOrdered(3, lambda (c, value) : -1 * value)]\n  \n  if cuisine == predictions[0]:\n    sum_correct += 1\n    \n  if cuisine in predictions:\n    sum_in_top_3 += 1\n    \n  # print \"INGREDIENTS: %s\" % \", \".join(recipe)\n  # print \"TOP 3 PREDICTIONS: %s\" % \", \".join([c for c, value in predictions])\n  # print \"ACTUAL: %s\" % cuisine\n  # print \"\"\n\nl = test_recipes_RDD.count()\n\nprint \"Accuracy: %s\" % round(float(sum_correct) / l, 3)\nprint \"In Top 3: %s\" % round(float(sum_in_top_3) / l, 3)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"food_project","notebookId":2338010650736672},"nbformat":4,"nbformat_minor":0}
